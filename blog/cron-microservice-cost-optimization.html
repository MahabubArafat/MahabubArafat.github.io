<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="How I eliminated wasteful database operations and built a scalable cron system that saved 30% on AWS costs while improving performance. The story of smart caching and microservice architecture.">
    <meta name="keywords" content="Microservices, AWS Cost Optimization, Cron Jobs, Redis, Database Optimization, ShareTrip, Architecture, Scalability">
    <meta name="author" content="Mahabub Alam Arafat">
    <meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
    
    <!-- Open Graph Meta Tags -->
    <meta property="og:title" content="From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice">
    <meta property="og:description" content="How I eliminated wasteful database operations and built a scalable cron system that saved 30% on AWS costs while improving performance. The story of smart caching and microservice architecture.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://mahabubarafat.online/blog/cron-microservice-cost-optimization.html">
    <meta property="og:site_name" content="Mahabub Alam Arafat - Portfolio">
    
    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice">
    <meta name="twitter:description" content="How I eliminated wasteful database operations and built a scalable cron system that saved 30% on AWS costs while improving performance. The story of smart caching and microservice architecture.">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {{"@context": "https://schema.org","@type": "BlogPosting","headline": "From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice","description": "How I eliminated wasteful database operations and built a scalable cron system that saved 30% on AWS costs while improving performance. The story of smart caching and microservice architecture.","author": {{"@type": "Person","name": "Mahabub Alam Arafat"}},"datePublished": "2025-01-17","dateModified": "2025-01-17","publisher": {{"@type": "Person","name": "Mahabub Alam Arafat"}},"mainEntityOfPage": {{"@type": "WebPage","@id": "https://mahabubarafat.online/blog/cron-microservice-cost-optimization.html"}},"keywords": ["Microservices", "AWS Cost Optimization", "Cron Jobs", "Redis", "Database Optimization", "ShareTrip", "Architecture", "Scalability"]}}
    </script>
    
    <title>From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice | Mahabub Alam Arafat</title>
    <link rel="canonical" href="https://mahabubarafat.online/blog/cron-microservice-cost-optimization.html">
    
    <!-- Google AdSense Code -->
    <meta name="google-adsense-account" content="ca-pub-6705222517983610">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6705222517983610"
            crossorigin="anonymous"></script>
    
    <link rel="stylesheet" href="blog_styling.css">
</head>
<body>
    <header>
        <h1>Mahabub Alam Arafat</h1>
        <p>Software Engineer - Blog</p>
    </header>

    <div class="container">
        <a href="../index.html#blog" class="back-link">‚Üê Back to Portfolio</a>
        
        <article>
            <header class="article-header">
                <h1 class="article-title">From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice</h1>
                <div class="article-meta">
                    <div class="meta-item">
                        <span>üìÖ</span>
                        <time datetime="2025-01-17">January 17, 2025</time>
                    </div>
                    <div class="meta-item">
                        <span>‚è±Ô∏è</span>
                        <span>14 min read</span>
                    </div>
                    <div class="meta-item">
                        <span>üè∑Ô∏è</span>
                        <span>Architecture</span>
                    </div>
                </div>
            </header>

            <div class="content">
                
<h1>From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice</h1>

<p><strong>TL;DR:</strong> We were burning money on AWS RDS by storing 4,000-5,000 flight search results per international search, only to delete 99.9% of them later. With a 1000:1 search-to-booking ratio, we were essentially paying premium prices to store garbage. I built a centralized cron microservice that eliminated this waste, saved 30% on AWS costs, and improved performance. Here's the complete story.</p>

<h2>The Expensive Problem: When Analytics Costs More Than Revenue</h2>

<p>Picture this: Every time someone searches for an international flight on ShareTrip, we generate 400-500 search results from multiple providers. Sounds normal, right?</p>

<p><strong>Here's the crazy part:</strong> We were storing ALL of them in AWS RDS. Every single search result. For analytics.</p>

<h3>The Math That Made Everyone Cringe</h3>

<pre><code># The brutal reality
1 international flight search = 400-500 results
Each result = Multiple table insertions (flights, segments, pricing, etc.)
Total DB operations per search = 4,000-5,000 inserts

# Our conversion rate
Search to booking ratio = 1000:1
Meaning: 999 out of 1000 searches never convert to bookings

# The waste
999 searches √ó 4,500 average inserts = 4.5 million pointless DB operations
All stored in expensive AWS RDS
All deleted by a cron job at end of day ü§¶‚Äç‚ôÇÔ∏è</code></pre>

<p>> <strong>The Wake-Up Call:</strong> Our AWS RDS bill was growing faster than our revenue. We were literally paying premium prices to store data we'd delete within 24 hours.</p>

<h2>The Legacy Architecture: A Masterclass in Inefficiency</h2>
<div class="ad-container" style="margin: 30px 0; text-align: center;">
    <ins class="adsbygoogle"
         style="display:block; text-align:center;"
         data-ad-layout="in-article"
         data-ad-format="fluid"
         data-ad-client="ca-pub-6705222517983610"
         data-ad-slot="1879717900"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>


<h3>How It "Worked" Before</h3>

<pre><code>graph TD
    A[User Search] --> B[Generate 500 Results]
    B --> C[Insert ALL to RDS]
    C --> D[User Books 1 Flight]
    D --> E[Mark 1 Result as Booked]
    E --> F[End of Day Cron]
    F --> G[Extract Analytics from 499 Unbooked]
    G --> H[Update Analytics Table]
    H --> I[DELETE 499 Results]
    I --> J[üí∞ Money Burned]</code></pre>

<h3>The Daily Waste Cycle</h3>

<p><strong>Morning:</strong> Fresh RDS, ready for action</p>
<pre><code>-- Clean slate
SELECT COUNT(*) FROM flight_search_results; 
-- Result: 0</code></pre>

<p><strong>Throughout the Day:</strong> Accumulating expensive data</p>
<pre><code>-- After 1000 searches
SELECT COUNT(*) FROM flight_search_results; 
-- Result: 4,500,000 rows (mostly garbage)</code></pre>

<p><strong>End of Day Cron:</strong> The expensive cleanup</p>
<pre><code>-- The daily purge
DELETE FROM flight_search_results 
WHERE booking_id IS NULL; 
-- Deleted: 4,455,000 rows (99.9% of everything)

-- Update analytics
INSERT INTO search_analytics 
SELECT provider, route, count(*) 
FROM deleted_data;</code></pre>

<p><strong>The Cost:</strong></p>
<ul>
<li>RDS storage for millions of temporary rows</li>
<li>Compute for massive INSERT operations</li>
<li>Compute for massive DELETE operations</li>
<li>Network I/O for data that never mattered</li>
</ul>

<h2>The Eureka Moment: Why Store What You'll Delete?</h2>

<p>During our monolith-to-microservices migration, we introduced Redis caching for flight data. That's when it hit me:</p>

<p>> <strong>"If we're already caching flights in Redis for performance, why are we also storing them in RDS for deletion?"</strong></p>

<h3>The New Vision</h3>

<p>Instead of the expensive store-then-delete cycle:</p>

<p>1. <strong>Cache search results in Redis</strong> (we were already doing this)</p>
<p>2. <strong>Extract analytics directly from Redis</strong> (every 30 minutes)</p>
<p>3. <strong>Never touch RDS for temporary data</strong> (revolutionary concept, apparently)</p>
<p>4. <strong>Only store actual bookings in RDS</strong> (you know, the data that matters)</p>

<p>The team's reaction: <em>"Why didn't we think of this sooner?"</em></p>

<h2>Building the Centralized Cron Microservice</h2>
<div class="ad-container" style="margin: 30px 0; text-align: center;">
    <ins class="adsbygoogle"
         style="display:block; text-align:center;"
         data-ad-layout="in-article"
         data-ad-format="fluid"
         data-ad-client="ca-pub-6705222517983610"
         data-ad-slot="1879717900"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>


<h3>The Architecture Decision</h3>

<p>Instead of scattered cron jobs across different services, I proposed a <strong>centralized cron microservice</strong>:</p>

<pre><code>// The new architecture
@Injectable()
export class CronOrchestrator {
  private readonly jobs = new Map<string, CronJob>();
  
  constructor(
    private readonly redisService: RedisService,
    private readonly analyticsService: AnalyticsService,
    private readonly logger: Logger
  ) {}

  @Cron('*/30 * * * *') // Every 30 minutes
  async processFlightAnalytics() {
    await this.extractAnalyticsFromRedis();
    await this.cleanupProcessedData();
  }
}</code></pre>

<h3>Why Centralized Cron Jobs?</h3>

<p><strong>Before: Distributed Chaos</strong></p>
<ul>
<li>Flight service had its cleanup cron</li>
<li>Booking service had its reminder cron</li>
<li>Payment service had its reconciliation cron</li>
<li>Each service managing its own schedule</li>
<li>No visibility into job execution</li>
<li>Failures went unnoticed</li>
</ul>

<p><strong>After: Orchestrated Efficiency</strong></p>
<ul>
<li>Single service manages all scheduled tasks</li>
<li>Centralized logging and monitoring</li>
<li>Easy to scale and manage</li>
<li>Clear dependency management</li>
<li>Failure visibility and alerting</li>
</ul>

<h2>The Implementation: Smart Caching Strategy</h2>

<h3>Step 1: Redis-First Analytics Collection</h3>

<pre><code>@Injectable()
export class FlightAnalyticsProcessor {
  
  async extractAnalyticsFromRedis(): Promise<void> {
    const searchKeys = await this.redis.keys('flight_search:*');
    const analyticsData = new Map<string, SearchMetrics>();
    
    for (const key of searchKeys) {
      const searchData = await this.redis.get(key);
      const flightData = JSON.parse(searchData);
      
      // Extract analytics without touching RDS
      this.aggregateMetrics(flightData, analyticsData);
    }
    
    // Bulk update analytics table
    await this.updateAnalyticsTable(analyticsData);
    
    // Clean up processed Redis data
    await this.cleanupRedisData(searchKeys);
  }
  
  private aggregateMetrics(flightData: any, metrics: Map<string, SearchMetrics>) {
    const key = `${flightData.provider}_${flightData.route}`;
    const existing = metrics.get(key) || new SearchMetrics();
    
    existing.searchCount++;
    existing.averagePrice = this.calculateAverage(existing, flightData.price);
    existing.popularTimes.push(flightData.searchTime);
    
    metrics.set(key, existing);
  }
}</code></pre>

<h3>Step 2: Intelligent Data Lifecycle</h3>

<pre><code>// The new flow - no more RDS waste
class FlightSearchService {
  
  async searchFlights(criteria: SearchCriteria): Promise<FlightResult[]> {
    const searchId = generateSearchId();
    const results = await this.aggregateFromProviders(criteria);
    
    // Store in Redis with TTL (not RDS!)
    await this.redis.setex(
      `flight_search:${searchId}`, 
      3600, // 1 hour TTL
      JSON.stringify({
        searchId,
        criteria,
        results,
        timestamp: new Date(),
        provider: 'multiple'
      })
    );
    
    return results;
  }
  
  async bookFlight(searchId: string, flightId: string): Promise<Booking> {
    // Only NOW do we touch RDS
    const searchData = await this.redis.get(`flight_search:${searchId}`);
    const selectedFlight = this.findFlightById(searchData, flightId);
    
    // Store the actual booking (permanent data)
    const booking = await this.bookingRepository.save({
      ...selectedFlight,
      bookingId: generateBookingId(),
      status: 'confirmed'
    });
    
    return booking;
  }
}</code></pre>

<h2>The Results: Numbers That Made Everyone Happy</h2>

<h3>Cost Optimization Breakdown</h3>

<p><strong>Before the Microservice:</strong></p>
<pre><code>Daily RDS Operations:
- Inserts: 4.5M rows/day √ó $0.0001/operation = $450/day
- Storage: 4.5M rows √ó 2KB √ó $0.023/GB/month = $207/month  
- Deletes: 4.45M rows/day √ó $0.0001/operation = $445/day
- Network I/O: ~50GB/day √ó $0.01/GB = $0.50/day

Monthly RDS Cost: ~$27,000</code></pre>

<p><strong>After the Microservice:</strong></p>
<pre><code>Daily Operations:
- Redis operations: 4.5M √ó $0.000001 = $4.50/day
- Analytics inserts: ~1000 rows/day √ó $0.0001 = $0.10/day  
- Booking inserts: ~4.5 rows/day √ó $0.0001 = $0.0005/day
- Redis storage: 50GB √ó $0.02/GB = $1.00/day

Monthly Cost: ~$18,900
Savings: $8,100/month (30% reduction!)</code></pre>

<h3>Performance Improvements</h3>

<p><strong>üöÄ Speed Gains:</strong></p>
<ul>
<li>Search response time: 2.8s ‚Üí 1.9s (32% faster)</li>
<li>Analytics processing: 45 minutes ‚Üí 2 minutes (95% faster)</li>
<li>End-of-day cleanup: 2 hours ‚Üí 5 minutes (96% faster)</li>
</ul>

<p><strong>üìä Operational Benefits:</strong></p>
<ul>
<li>Zero downtime during migration</li>
<li>Reduced RDS load by 90%</li>
<li>Eliminated daily cleanup bottlenecks</li>
<li>Improved monitoring and alerting</li>
</ul>

<h2>The Technical Deep Dive: Microservice Architecture</h2>

<h3>Service Structure</h3>

<pre><code>// Centralized cron microservice structure
src/
‚îú‚îÄ‚îÄ cron/
‚îÇ   ‚îú‚îÄ‚îÄ cron.controller.ts      # Health checks and manual triggers
‚îÇ   ‚îú‚îÄ‚îÄ cron.service.ts         # Job orchestration
‚îÇ   ‚îî‚îÄ‚îÄ jobs/
‚îÇ       ‚îú‚îÄ‚îÄ flight-analytics.job.ts
‚îÇ       ‚îú‚îÄ‚îÄ booking-reminders.job.ts
‚îÇ       ‚îî‚îÄ‚îÄ payment-reconciliation.job.ts
‚îú‚îÄ‚îÄ processors/
‚îÇ   ‚îú‚îÄ‚îÄ analytics.processor.ts   # Redis ‚Üí Analytics logic
‚îÇ   ‚îú‚îÄ‚îÄ cleanup.processor.ts     # Data lifecycle management
‚îÇ   ‚îî‚îÄ‚îÄ notification.processor.ts
‚îî‚îÄ‚îÄ shared/
    ‚îú‚îÄ‚îÄ redis.service.ts
    ‚îú‚îÄ‚îÄ monitoring.service.ts
    ‚îî‚îÄ‚îÄ types/</code></pre>

<h3>Scalability Features</h3>

<pre><code>@Injectable()
export class ScalableCronService {
  
  // Distributed job execution
  @Cron('*/30 * * * *')
  async processAnalytics() {
    const jobId = `analytics_${Date.now()}`;
    
    // Prevent duplicate execution across instances
    const lock = await this.redis.setnx(`job_lock:${jobId}`, 'locked');
    if (!lock) {
      this.logger.log('Job already running on another instance');
      return;
    }
    
    try {
      await this.executeAnalyticsJob(jobId);
    } finally {
      await this.redis.del(`job_lock:${jobId}`);
    }
  }
  
  // Batch processing for large datasets
  async executeAnalyticsJob(jobId: string) {
    const batchSize = 1000;
    let processed = 0;
    
    while (true) {
      const batch = await this.getNextBatch(processed, batchSize);
      if (batch.length === 0) break;
      
      await this.processBatch(batch);
      processed += batch.length;
      
      // Progress tracking
      await this.updateJobProgress(jobId, processed);
    }
  }
}</code></pre>

<h2>Challenges and Solutions</h2>

<h3>Challenge 1: Data Consistency During Migration</h3>

<p><strong>Problem:</strong> How do we migrate without losing data or breaking analytics?</p>

<p><strong>Solution:</strong> Dual-write pattern with gradual migration</p>
<pre><code>// Transition period - write to both systems
async storeSearchResults(data: SearchData) {
  // New way (Redis)
  await this.redis.setex(`flight_search:${data.id}`, 3600, JSON.stringify(data));
  
  // Old way (RDS) - during migration only
  if (this.config.MIGRATION_MODE) {
    await this.legacyRepository.save(data);
  }
}</code></pre>

<h3>Challenge 2: Redis Memory Management</h3>

<p><strong>Problem:</strong> Redis running out of memory with large datasets</p>

<p><strong>Solution:</strong> Smart TTL and data compression</p>
<pre><code>// Intelligent data lifecycle
async storeWithSmartTTL(key: string, data: any) {
  const compressed = this.compressData(data);
  const ttl = this.calculateOptimalTTL(data.type);
  
  await this.redis.setex(key, ttl, compressed);
  
  // Monitor memory usage
  const memoryUsage = await this.redis.memory('usage', key);
  if (memoryUsage > this.config.MAX_KEY_SIZE) {
    await this.archiveToS3(key, data);
    await this.redis.del(key);
  }
}</code></pre>

<h3>Challenge 3: Monitoring and Observability</h3>

<p><strong>Problem:</strong> How do we know if cron jobs are working properly?</p>

<p><strong>Solution:</strong> Comprehensive monitoring with Grafana dashboards</p>
<pre><code>@Injectable()
export class CronMonitoringService {
  
  async recordJobExecution(jobName: string, duration: number, status: 'success' | 'failed') {
    // Metrics for Grafana
    this.metricsService.histogram('cron_job_duration', duration, { job: jobName });
    this.metricsService.counter('cron_job_executions', 1, { job: jobName, status });
    
    // Alerting
    if (status === 'failed') {
      await this.alertingService.sendAlert({
        severity: 'high',
        message: `Cron job ${jobName} failed`,
        duration
      });
    }
  }
}</code></pre>

<h2>The Team Impact: Beyond Cost Savings</h2>

<h3>Developer Experience Improvements</h3>

<p><strong>Before:</strong></p>
<ul>
<li>"The analytics cron failed again" üò§</li>
<li>"Why is RDS so slow today?" üêå</li>
<li>"We're over budget on AWS again" üí∏</li>
</ul>

<p><strong>After:</strong></p>
<ul>
<li>"Analytics are updating every 30 minutes automatically" ‚úÖ</li>
<li>"RDS performance is consistently good" üöÄ</li>
<li>"We're under budget with better performance" üí∞</li>
</ul>

<h3>Operational Excellence</h3>

<pre><code># Monitoring dashboard metrics
Cron Job Success Rate: 99.8%
Average Job Duration: 2.3 minutes (down from 45 minutes)
Failed Jobs This Month: 2 (down from 45)
Cost per Analytics Update: $0.02 (down from $12.50)</code></pre>

<h2>Lessons Learned: Architecture Wisdom</h2>

<h3>1. Question the Status Quo</h3>
<p>Just because "we've always done it this way" doesn't mean it's right. Sometimes the biggest improvements come from asking "why are we doing this?"</p>

<h3>2. Cache-First Thinking</h3>
<p>If you're already caching data for performance, consider using that cache for other purposes too. Don't duplicate storage unnecessarily.</p>

<h3>3. Centralized vs Distributed</h3>
<p>Centralized cron management provides better visibility and control than scattered job scheduling across services.</p>

<h3>4. Measure Everything</h3>
<p>Without metrics, you can't prove improvements. Track costs, performance, and reliability before and after changes.</p>

<h3>5. Migration Strategy Matters</h3>
<p>Dual-write patterns and gradual migration reduce risk when changing critical data flows.</p>

<h2>What's Next: Future Improvements</h2>

<p>I'm working on v2 of the cron microservice with:</p>

<ul>
<li><strong>AI-powered job scheduling</strong> - Optimize timing based on system load</li>
<li><strong>Cross-region job distribution</strong> - Better resilience and performance</li>
<li><strong>Dynamic scaling</strong> - Auto-scale based on workload</li>
<li><strong>Advanced analytics</strong> - Predictive insights on data patterns</li>
<li><strong>Integration with Kubernetes CronJobs</strong> - Cloud-native scheduling</li>
</ul>

<h2>The Technical Takeaway</h2>

<p>This wasn't just about saving money - it was about building smarter systems. The key insights:</p>

<p>1. <strong>Data lifecycle thinking</strong> - Not all data needs permanent storage</p>
<p>2. <strong>Cache optimization</strong> - Use Redis for more than just performance</p>
<p>3. <strong>Centralized orchestration</strong> - Better than distributed chaos</p>
<p>4. <strong>Cost-conscious architecture</strong> - Every database operation has a price</p>
<p>5. <strong>Monitoring-first design</strong> - Observability isn't optional</p>

<h2>Try This Approach</h2>

<p>If you're dealing with:</p>
<ul>
<li>High AWS RDS costs from temporary data</li>
<li>Scattered cron jobs across services</li>
<li>Analytics processing bottlenecks</li>
<li>Poor visibility into scheduled tasks</li>
</ul>

<p>Consider the centralized cron microservice pattern. Start small:</p>

<p>1. <strong>Audit your current cron jobs</strong> - What's running where?</p>
<p>2. <strong>Identify wasteful data patterns</strong> - What gets stored then deleted?</p>
<p>3. <strong>Leverage existing caches</strong> - Can Redis do double duty?</p>
<p>4. <strong>Build monitoring first</strong> - You need visibility</p>
<p>5. <strong>Migrate gradually</strong> - Dual-write during transition</p>

<hr>

<p><strong>The Bottom Line:</strong> Sometimes the best optimization isn't making things faster - it's eliminating unnecessary work entirely. We saved 30% on AWS costs not by optimizing database queries, but by questioning why we were using the database at all.</p>

<p><em>Building cost-effective architectures? I'd love to hear about your optimization strategies. Connect with me on <a href="https://linkedin.com/in/mahabubarafat">LinkedIn</a> - let's discuss smart architecture patterns and cost optimization techniques.</em></p>

            </div>

            <!-- End of Article Ad -->
            <div class="ad-container" style="margin: 40px 0; text-align: center;">
                <ins class="adsbygoogle"
                     style="display:block; text-align:center;"
                     data-ad-layout="in-article"
                     data-ad-format="fluid"
                     data-ad-client="ca-pub-6705222517983610"
                     data-ad-slot="1879717900"></ins>
                <script>
                     (adsbygoogle = window.adsbygoogle || []).push({});
                </script>
            </div>

            <div class="social-share">
                <h3>Share this article</h3>
                <div class="share-buttons">
                    <a href="https://linkedin.com/shareArticle?mini=true&url=https://mahabubarafat.online/blog/cron-microservice-cost-optimization.html&title=From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice" target="_blank" class="share-btn share-linkedin">Share on LinkedIn</a>
                    <a href="https://twitter.com/intent/tweet?url=https://mahabubarafat.online/blog/cron-microservice-cost-optimization.html&text=From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice" target="_blank" class="share-btn share-twitter">Share on Twitter</a>
                </div>
            </div>

            <div class="author-bio">
                <h3>About the Author</h3>
                <p><strong>Mahabub Alam Arafat</strong> is a Software Engineer at ShareTrip with 2+ years of production experience. He specializes in backend development, API optimization, and turning legacy systems into modern, maintainable code.</p>
                <p><a href="../index.html#contact">Get in touch</a> | <a href="https://linkedin.com/in/mahabubarafat" target="_blank">LinkedIn</a> | <a href="https://github.com/MahabubArafat" target="_blank">GitHub</a></p>
            </div>
        </article>
    </div>

    <footer>
        <p>¬© 2025 Mahabub Alam Arafat. All rights reserved.</p>
    </footer>
</body>
</html>