<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="How I eliminated wasteful database operations and built a scalable cron system that saved 30% on AWS costs while improving performance. The story of smart caching and microservice architecture.">
    <meta name="keywords" content="Microservices, AWS Cost Optimization, Cron Jobs, Redis, Database Optimization, ShareTrip, Architecture, Scalability">
    <meta name="author" content="Mahabub Alam Arafat">
    <meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
    
    <!-- Open Graph Meta Tags -->
    <meta property="og:title" content="From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice">
    <meta property="og:description" content="How I eliminated wasteful database operations and built a scalable cron system that saved 30% on AWS costs while improving performance. The story of smart caching and microservice architecture.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://mahabubarafat.online/blog/cron-microservice-cost-optimization.html">
    <meta property="og:site_name" content="Mahabub Alam Arafat - Portfolio">
    
    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice">
    <meta name="twitter:description" content="How I eliminated wasteful database operations and built a scalable cron system that saved 30% on AWS costs while improving performance. The story of smart caching and microservice architecture.">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice",
      "description": "How I eliminated wasteful database operations and built a scalable cron system that saved 30% on AWS costs while improving performance. The story of smart caching and microservice architecture.",
      "author": {
        "@type": "Person",
        "name": "Mahabub Alam Arafat"
      },
      "datePublished": "2025-01-17",
      "dateModified": "2025-01-17",
      "publisher": {
        "@type": "Person",
        "name": "Mahabub Alam Arafat"
      },
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://mahabubarafat.online/blog/cron-microservice-cost-optimization.html"
      },
      "keywords": ["Microservices", "AWS Cost Optimization", "Cron Jobs", "Redis", "Database Optimization", "ShareTrip", "Architecture", "Scalability"]
    }
    </script>
    
    <title>From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice | Mahabub Alam Arafat</title>
    <link rel="canonical" href="https://mahabubarafat.online/blog/cron-microservice-cost-optimization.html">
    <link rel="stylesheet" href="blog_styling.css">
</head>
<body>
    <header>
        <h1>Mahabub Alam Arafat</h1>
        <p>Software Engineer - Blog</p>
    </header>

    <div class="container">
        <a href="../index.html#blog" class="back-link">‚Üê Back to Portfolio</a>
        
        <article>
            <header class="article-header">
                <h1 class="article-title">From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice</h1>
                <div class="article-meta">
                    <div class="meta-item">
                        <span>üìÖ</span>
                        <time datetime="2025-01-17">January 17, 2025</time>
                    </div>
                    <div class="meta-item">
                        <span>‚è±Ô∏è</span>
                        <span>14 min read</span>
                    </div>
                    <div class="meta-item">
                        <span>üè∑Ô∏è</span>
                        <span>Architecture</span>
                    </div>
                </div>
            </header>

            <div class="content">
                
<h1>From 30% Cost Savings to Zero Downtime: Building a Centralized Cron Microservice</h1>

<strong>TL;DR:</strong> We were burning money on AWS RDS by storing 4,000-5,000 flight search results per international search, only to delete 99.9% of them later. With a 1000:1 search-to-booking ratio, we were essentially paying premium prices to store garbage. I built a centralized cron microservice that eliminated this waste, saved 30% on AWS costs, and improved performance. Here's the complete story.

<h2>The Expensive Problem: When Analytics Costs More Than Revenue</h2>

<p>Picture this: Every time someone searches for an international flight on ShareTrip, we generate 400-500 search results from multiple providers. Sounds normal, right?</p>

<strong>Here's the crazy part:</strong> We were storing ALL of them in AWS RDS. Every single search result. For analytics. 

<h3>The Math That Made Everyone Cringe</h3>

<pre><code><h1>The brutal reality</h1>
<p>1 international flight search = 400-500 results</p>
<p>Each result = Multiple table insertions (flights, segments, pricing, etc.)</p>
<p>Total DB operations per search = 4,000-5,000 inserts</p>

<h1>Our conversion rate</h1>
<p>Search to booking ratio = 1000:1</p>
<p>Meaning: 999 out of 1000 searches never convert to bookings</p>

<h1>The waste</h1>
<p>999 searches √ó 4,500 average inserts = 4.5 million pointless DB operations</p>
<p>All stored in expensive AWS RDS</p>
<p>All deleted by a cron job at end of day ü§¶‚Äç‚ôÇÔ∏è</p>
</code></pre>

<p>> <strong>The Wake-Up Call:</strong> Our AWS RDS bill was growing faster than our revenue. We were literally paying premium prices to store data we'd delete within 24 hours.</p>

<h2>The Legacy Architecture: A Masterclass in Inefficiency</h2>

<h3>How It "Worked" Before</h3>

<pre><code>graph TD
<p>A[User Search] --> B[Generate 500 Results]</p>
<p>B --> C[Insert ALL to RDS]</p>
<p>C --> D[User Books 1 Flight]</p>
<p>D --> E[Mark 1 Result as Booked]</p>
<p>E --> F[End of Day Cron]</p>
<p>F --> G[Extract Analytics from 499 Unbooked]</p>
<p>G --> H[Update Analytics Table]</p>
<p>H --> I[DELETE 499 Results]</p>
<p>I --> J[üí∞ Money Burned]</p>
</code></pre>

<h3>The Daily Waste Cycle</h3>

<strong>Morning:</strong> Fresh RDS, ready for action
<pre><code>-- Clean slate
<p>SELECT COUNT(*) FROM flight_search_results;</p>
<p>-- Result: 0</p>
</code></pre>

<strong>Throughout the Day:</strong> Accumulating expensive data
<pre><code>-- After 1000 searches
<p>SELECT COUNT(*) FROM flight_search_results;</p>
<p>-- Result: 4,500,000 rows (mostly garbage)</p>
</code></pre>

<strong>End of Day Cron:</strong> The expensive cleanup
<pre><code>-- The daily purge
<p>DELETE FROM flight_search_results</p>
<p>WHERE booking_id IS NULL;</p>
<p>-- Deleted: 4,455,000 rows (99.9% of everything)</p>

<p>-- Update analytics</p>
<p>INSERT INTO search_analytics</p>
<p>SELECT provider, route, count(*)</p>
<p>FROM deleted_data;</p>
</code></pre>

<strong>The Cost:</strong> 
<ul>
<li>RDS storage for millions of temporary rows</li>
<li>Compute for massive INSERT operations</li>
<li>Compute for massive DELETE operations</li>
<li>Network I/O for data that never mattered</li>
</ul>

<h2>The Eureka Moment: Why Store What You'll Delete?</h2>

<p>During our monolith-to-microservices migration, we introduced Redis caching for flight data. That's when it hit me:</p>

<p>> <strong>"If we're already caching flights in Redis for performance, why are we also storing them in RDS for deletion?"</strong></p>

<h3>The New Vision</h3>

<p>Instead of the expensive store-then-delete cycle:</p>

<p>1. <strong>Cache search results in Redis</strong> (we were already doing this)</p>
<p>2. <strong>Extract analytics directly from Redis</strong> (every 30 minutes)</p>
<p>3. <strong>Never touch RDS for temporary data</strong> (revolutionary concept, apparently)</p>
<p>4. <strong>Only store actual bookings in RDS</strong> (you know, the data that matters)</p>

<p>The team's reaction: <em>"Why didn't we think of this sooner?"</em></p>

<h2>Building the Centralized Cron Microservice</h2>

<h3>The Architecture Decision</h3>

<p>Instead of scattered cron jobs across different services, I proposed a <strong>centralized cron microservice</strong>:</p>

<pre><code>// The new architecture
<p>@Injectable()</p>
<p>export class CronOrchestrator {</p>
<p>private readonly jobs = new Map<string, CronJob>();</p>
  
<p>constructor(</p>
<p>private readonly redisService: RedisService,</p>
<p>private readonly analyticsService: AnalyticsService,</p>
<p>private readonly logger: Logger</p>
<p>) {}</p>

<p>@Cron('<em>/30 </em> <em> </em> *') // Every 30 minutes</p>
<p>async processFlightAnalytics() {</p>
<p>await this.extractAnalyticsFromRedis();</p>
<p>await this.cleanupProcessedData();</p>
<p>}</p>
<p>}</p>
</code></pre>

<h3>Why Centralized Cron Jobs?</h3>

<strong>Before: Distributed Chaos</strong>
<ul>
<li>Flight service had its cleanup cron</li>
<li>Booking service had its reminder cron</li>
<li>Payment service had its reconciliation cron</li>
<li>Each service managing its own schedule</li>
<li>No visibility into job execution</li>
<li>Failures went unnoticed</li>
</ul>

<strong>After: Orchestrated Efficiency</strong>
<ul>
<li>Single service manages all scheduled tasks</li>
<li>Centralized logging and monitoring</li>
<li>Easy to scale and manage</li>
<li>Clear dependency management</li>
<li>Failure visibility and alerting</li>
</ul>

<h2>The Implementation: Smart Caching Strategy</h2>

<h3>Step 1: Redis-First Analytics Collection</h3>

<pre><code>@Injectable()
<p>export class FlightAnalyticsProcessor {</p>
  
<p>async extractAnalyticsFromRedis(): Promise<void> {</p>
<p>const searchKeys = await this.redis.keys('flight_search:*');</p>
<p>const analyticsData = new Map<string, SearchMetrics>();</p>
    
<p>for (const key of searchKeys) {</p>
<p>const searchData = await this.redis.get(key);</p>
<p>const flightData = JSON.parse(searchData);</p>
      
<p>// Extract analytics without touching RDS</p>
<p>this.aggregateMetrics(flightData, analyticsData);</p>
<p>}</p>
    
<p>// Bulk update analytics table</p>
<p>await this.updateAnalyticsTable(analyticsData);</p>
    
<p>// Clean up processed Redis data</p>
<p>await this.cleanupRedisData(searchKeys);</p>
<p>}</p>
  
<p>private aggregateMetrics(flightData: any, metrics: Map<string, SearchMetrics>) {</p>
<p>const key = <code>${flightData.provider}_${flightData.route}</code>;</p>
<p>const existing = metrics.get(key) || new SearchMetrics();</p>
    
<p>existing.searchCount++;</p>
<p>existing.averagePrice = this.calculateAverage(existing, flightData.price);</p>
<p>existing.popularTimes.push(flightData.searchTime);</p>
    
<p>metrics.set(key, existing);</p>
<p>}</p>
<p>}</p>
</code></pre>

<h3>Step 2: Intelligent Data Lifecycle</h3>

<pre><code>// The new flow - no more RDS waste
<p>class FlightSearchService {</p>
  
<p>async searchFlights(criteria: SearchCriteria): Promise<FlightResult[]> {</p>
<p>const searchId = generateSearchId();</p>
<p>const results = await this.aggregateFromProviders(criteria);</p>
    
<p>// Store in Redis with TTL (not RDS!)</p>
<p>await this.redis.setex(</p>
      <code>flight_search:${searchId}</code>, 
<p>3600, // 1 hour TTL</p>
<p>JSON.stringify({</p>
<p>searchId,</p>
<p>criteria,</p>
<p>results,</p>
<p>timestamp: new Date(),</p>
<p>provider: 'multiple'</p>
<p>})</p>
<p>);</p>
    
<p>return results;</p>
<p>}</p>
  
<p>async bookFlight(searchId: string, flightId: string): Promise<Booking> {</p>
<p>// Only NOW do we touch RDS</p>
<p>const searchData = await this.redis.get(<code>flight_search:${searchId}</code>);</p>
<p>const selectedFlight = this.findFlightById(searchData, flightId);</p>
    
<p>// Store the actual booking (permanent data)</p>
<p>const booking = await this.bookingRepository.save({</p>
<p>...selectedFlight,</p>
<p>bookingId: generateBookingId(),</p>
<p>status: 'confirmed'</p>
<p>});</p>
    
<p>return booking;</p>
<p>}</p>
<p>}</p>
</code></pre>

<h2>The Results: Numbers That Made Everyone Happy</h2>

<h3>Cost Optimization Breakdown</h3>

<strong>Before the Microservice:</strong>
<pre><code>Daily RDS Operations:
<ul>
<li>Inserts: 4.5M rows/day √ó $0.0001/operation = $450/day</li>
<li>Storage: 4.5M rows √ó 2KB √ó $0.023/GB/month = $207/month</li>
<li>Deletes: 4.45M rows/day √ó $0.0001/operation = $445/day</li>
<li>Network I/O: ~50GB/day √ó $0.01/GB = $0.50/day</li>
</ul>

<p>Monthly RDS Cost: ~$27,000</p>
</code></pre>

<strong>After the Microservice:</strong>
<pre><code>Daily Operations:
<ul>
<li>Redis operations: 4.5M √ó $0.000001 = $4.50/day</li>
<li>Analytics inserts: ~1000 rows/day √ó $0.0001 = $0.10/day</li>
<li>Booking inserts: ~4.5 rows/day √ó $0.0001 = $0.0005/day</li>
<li>Redis storage: 50GB √ó $0.02/GB = $1.00/day</li>
</ul>

<p>Monthly Cost: ~$18,900</p>
<p>Savings: $8,100/month (30% reduction!)</p>
</code></pre>

<h3>Performance Improvements</h3>

<strong>üöÄ Speed Gains:</strong>
<ul>
<li>Search response time: 2.8s ‚Üí 1.9s (32% faster)</li>
<li>Analytics processing: 45 minutes ‚Üí 2 minutes (95% faster)</li>
<li>End-of-day cleanup: 2 hours ‚Üí 5 minutes (96% faster)</li>
</ul>

<strong>üìä Operational Benefits:</strong>
<ul>
<li>Zero downtime during migration</li>
<li>Reduced RDS load by 90%</li>
<li>Eliminated daily cleanup bottlenecks</li>
<li>Improved monitoring and alerting</li>
</ul>

<h2>The Technical Deep Dive: Microservice Architecture</h2>

<h3>Service Structure</h3>

<pre><code>// Centralized cron microservice structure
<p>src/</p>
<p>‚îú‚îÄ‚îÄ cron/</p>
<p>‚îÇ   ‚îú‚îÄ‚îÄ cron.controller.ts      # Health checks and manual triggers</p>
<p>‚îÇ   ‚îú‚îÄ‚îÄ cron.service.ts         # Job orchestration</p>
<p>‚îÇ   ‚îî‚îÄ‚îÄ jobs/</p>
<p>‚îÇ       ‚îú‚îÄ‚îÄ flight-analytics.job.ts</p>
<p>‚îÇ       ‚îú‚îÄ‚îÄ booking-reminders.job.ts</p>
<p>‚îÇ       ‚îî‚îÄ‚îÄ payment-reconciliation.job.ts</p>
<p>‚îú‚îÄ‚îÄ processors/</p>
<p>‚îÇ   ‚îú‚îÄ‚îÄ analytics.processor.ts   # Redis ‚Üí Analytics logic</p>
<p>‚îÇ   ‚îú‚îÄ‚îÄ cleanup.processor.ts     # Data lifecycle management</p>
<p>‚îÇ   ‚îî‚îÄ‚îÄ notification.processor.ts</p>
<p>‚îî‚îÄ‚îÄ shared/</p>
<p>‚îú‚îÄ‚îÄ redis.service.ts</p>
<p>‚îú‚îÄ‚îÄ monitoring.service.ts</p>
<p>‚îî‚îÄ‚îÄ types/</p>
</code></pre>

<h3>Scalability Features</h3>

<pre><code>@Injectable()
<p>export class ScalableCronService {</p>
  
<p>// Distributed job execution</p>
<p>@Cron('<em>/30 </em> <em> </em> *')</p>
<p>async processAnalytics() {</p>
<p>const jobId = <code>analytics_${Date.now()}</code>;</p>
    
<p>// Prevent duplicate execution across instances</p>
<p>const lock = await this.redis.setnx(<code>job_lock:${jobId}</code>, 'locked');</p>
<p>if (!lock) {</p>
<p>this.logger.log('Job already running on another instance');</p>
<p>return;</p>
<p>}</p>
    
<p>try {</p>
<p>await this.executeAnalyticsJob(jobId);</p>
<p>} finally {</p>
<p>await this.redis.del(<code>job_lock:${jobId}</code>);</p>
<p>}</p>
<p>}</p>
  
<p>// Batch processing for large datasets</p>
<p>async executeAnalyticsJob(jobId: string) {</p>
<p>const batchSize = 1000;</p>
<p>let processed = 0;</p>
    
<p>while (true) {</p>
<p>const batch = await this.getNextBatch(processed, batchSize);</p>
<p>if (batch.length === 0) break;</p>
      
<p>await this.processBatch(batch);</p>
<p>processed += batch.length;</p>
      
<p>// Progress tracking</p>
<p>await this.updateJobProgress(jobId, processed);</p>
<p>}</p>
<p>}</p>
<p>}</p>
</code></pre>

<h2>Challenges and Solutions</h2>

<h3>Challenge 1: Data Consistency During Migration</h3>

<strong>Problem:</strong> How do we migrate without losing data or breaking analytics?

<strong>Solution:</strong> Dual-write pattern with gradual migration
<pre><code>// Transition period - write to both systems
<p>async storeSearchResults(data: SearchData) {</p>
<p>// New way (Redis)</p>
<p>await this.redis.setex(<code>flight_search:${data.id}</code>, 3600, JSON.stringify(data));</p>
  
<p>// Old way (RDS) - during migration only</p>
<p>if (this.config.MIGRATION_MODE) {</p>
<p>await this.legacyRepository.save(data);</p>
<p>}</p>
<p>}</p>
</code></pre>

<h3>Challenge 2: Redis Memory Management</h3>

<strong>Problem:</strong> Redis running out of memory with large datasets

<strong>Solution:</strong> Smart TTL and data compression
<pre><code>// Intelligent data lifecycle
<p>async storeWithSmartTTL(key: string, data: any) {</p>
<p>const compressed = this.compressData(data);</p>
<p>const ttl = this.calculateOptimalTTL(data.type);</p>
  
<p>await this.redis.setex(key, ttl, compressed);</p>
  
<p>// Monitor memory usage</p>
<p>const memoryUsage = await this.redis.memory('usage', key);</p>
<p>if (memoryUsage > this.config.MAX_KEY_SIZE) {</p>
<p>await this.archiveToS3(key, data);</p>
<p>await this.redis.del(key);</p>
<p>}</p>
<p>}</p>
</code></pre>

<h3>Challenge 3: Monitoring and Observability</h3>

<strong>Problem:</strong> How do we know if cron jobs are working properly?

<strong>Solution:</strong> Comprehensive monitoring with Grafana dashboards
<pre><code>@Injectable()
<p>export class CronMonitoringService {</p>
  
<p>async recordJobExecution(jobName: string, duration: number, status: 'success' | 'failed') {</p>
<p>// Metrics for Grafana</p>
<p>this.metricsService.histogram('cron_job_duration', duration, { job: jobName });</p>
<p>this.metricsService.counter('cron_job_executions', 1, { job: jobName, status });</p>
    
<p>// Alerting</p>
<p>if (status === 'failed') {</p>
<p>await this.alertingService.sendAlert({</p>
<p>severity: 'high',</p>
<p>message: <code>Cron job ${jobName} failed</code>,</p>
<p>duration</p>
<p>});</p>
<p>}</p>
<p>}</p>
<p>}</p>
</code></pre>

<h2>The Team Impact: Beyond Cost Savings</h2>

<h3>Developer Experience Improvements</h3>

<strong>Before:</strong> 
<ul>
<li>"The analytics cron failed again" üò§</li>
<li>"Why is RDS so slow today?" üêå</li>
<li>"We're over budget on AWS again" üí∏</li>
</ul>

<strong>After:</strong>
<ul>
<li>"Analytics are updating every 30 minutes automatically" ‚úÖ</li>
<li>"RDS performance is consistently good" üöÄ</li>
<li>"We're under budget with better performance" üí∞</li>
</ul>

<h3>Operational Excellence</h3>

<pre><code><h1>Monitoring dashboard metrics</h1>
<p>Cron Job Success Rate: 99.8%</p>
<p>Average Job Duration: 2.3 minutes (down from 45 minutes)</p>
<p>Failed Jobs This Month: 2 (down from 45)</p>
<p>Cost per Analytics Update: $0.02 (down from $12.50)</p>
</code></pre>

<h2>Lessons Learned: Architecture Wisdom</h2>

<h3>1. Question the Status Quo</h3>
<p>Just because "we've always done it this way" doesn't mean it's right. Sometimes the biggest improvements come from asking "why are we doing this?"</p>

<h3>2. Cache-First Thinking</h3>
<p>If you're already caching data for performance, consider using that cache for other purposes too. Don't duplicate storage unnecessarily.</p>

<h3>3. Centralized vs Distributed</h3>
<p>Centralized cron management provides better visibility and control than scattered job scheduling across services.</p>

<h3>4. Measure Everything</h3>
<p>Without metrics, you can't prove improvements. Track costs, performance, and reliability before and after changes.</p>

<h3>5. Migration Strategy Matters</h3>
<p>Dual-write patterns and gradual migration reduce risk when changing critical data flows.</p>

<h2>What's Next: Future Improvements</h2>

<p>I'm working on v2 of the cron microservice with:</p>

<ul>
<li><strong>AI-powered job scheduling</strong> - Optimize timing based on system load</li>
<li><strong>Cross-region job distribution</strong> - Better resilience and performance</li>
<li><strong>Dynamic scaling</strong> - Auto-scale based on workload</li>
<li><strong>Advanced analytics</strong> - Predictive insights on data patterns</li>
<li><strong>Integration with Kubernetes CronJobs</strong> - Cloud-native scheduling</li>
</ul>

<h2>The Technical Takeaway</h2>

<p>This wasn't just about saving money - it was about building smarter systems. The key insights:</p>

<p>1. <strong>Data lifecycle thinking</strong> - Not all data needs permanent storage</p>
<p>2. <strong>Cache optimization</strong> - Use Redis for more than just performance</p>
<p>3. <strong>Centralized orchestration</strong> - Better than distributed chaos</p>
<p>4. <strong>Cost-conscious architecture</strong> - Every database operation has a price</p>
<p>5. <strong>Monitoring-first design</strong> - Observability isn't optional</p>

<h2>Try This Approach</h2>

<p>If you're dealing with:</p>
<ul>
<li>High AWS RDS costs from temporary data</li>
<li>Scattered cron jobs across services</li>
<li>Analytics processing bottlenecks</li>
<li>Poor visibility into scheduled tasks</li>
</ul>

<p>Consider the centralized cron microservice pattern. Start small:</p>

<p>1. <strong>Audit your current cron jobs</strong> - What's running where?</p>
<p>2. <strong>Identify wasteful data patterns</strong> - What gets stored then deleted?</p>
<p>3. <strong>Leverage existing caches</strong> - Can Redis do double duty?</p>
<p>4. <strong>Build monitoring first</strong> - You need visibility</p>
<p>5. <strong>Migrate gradually</strong> - Dual-write during transition</p>

<p>---</p>

<strong>The Bottom Line:</strong> Sometimes the best optimization isn't making things faster - it's eliminating unnecessary work entirely. We saved 30% on AWS costs not by optimizing database queries, but by questioning why we were using the database at all.

<em>Building cost-effective architectures? I'd love to hear about your optimization strategies. Connect with me on <a href="https://linkedin.com/in/mahabubarafat">LinkedIn</a> - let's discuss smart architecture patterns and cost optimization techniques.</em>

            </div>

            <div class="social-share">
                <h3>Share this article</h3>
                <div class="share-buttons">
                    <a href="https://linkedin.com/shareArticle?mini=true&url=https://mahabubarafat.online/blog/cron-microservice-cost-optimization.html&title=From%2030%%20Cost%20Savings%20to%20Zero%20Downtime:%20Building%20a%20Centralized%20Cron%20Microservice" target="_blank" class="share-btn share-linkedin">Share on LinkedIn</a>
                    <a href="https://twitter.com/intent/tweet?url=https://mahabubarafat.online/blog/cron-microservice-cost-optimization.html&text=From%2030%%20Cost%20Savings%20to%20Zero%20Downtime:%20Building%20a%20Centralized%20Cron%20Microservice" target="_blank" class="share-btn share-twitter">Share on Twitter</a>
                </div>
            </div>

            <div class="author-bio">
                <h3>About the Author</h3>
                <p><strong>Mahabub Alam Arafat</strong> is a Software Engineer at ShareTrip with 2+ years of production experience. He specializes in backend development, API optimization, and turning legacy systems into modern, maintainable code.</p>
                <p><a href="../index.html#contact">Get in touch</a> | <a href="https://linkedin.com/in/mahabubarafat" target="_blank">LinkedIn</a> | <a href="https://github.com/MahabubArafat" target="_blank">GitHub</a></p>
            </div>
        </article>
    </div>

    <footer>
        <p>¬© 2025 Mahabub Alam Arafat. All rights reserved.</p>
    </footer>
</body>
</html>